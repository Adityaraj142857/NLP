# N-Gram Language Modeling & Domain Shift Analysis

## ğŸ“Œ Project Overview
This project implements **N-gram Language Models (Unigram, Bigram, Trigram)** from scratch for **CS60075 Natural Language Processing (Assignment 1)**[cite: 1].

The goal is to predict the next word in a sequence based on probability estimates derived from a training corpus[cite: 16]. The models are trained on Wikipedia articles and evaluated on three different domains to analyze the impact of **domain shift** on model performance[cite: 41].

Key features include:
* **Maximum Likelihood Estimation (MLE)** for probability calculation[cite: 51].
* **Smoothing Techniques**: Implementation of Laplace (Add-1)  and Good-Turing Discounting [cite: 65] to handle unseen n-grams.
* **Perplexity Evaluation**: Comparing model performance across In-Domain (Wikipedia) and Out-of-Domain (Scientific, Financial) datasets[cite: 104, 105].

---

## ğŸ“‚ Repository Structure

The codebase is modularized for efficiency and readability:

```text
â”œâ”€â”€ data/                   # Dataset folder (Place CSVs here)
â”‚   â”œâ”€â”€ wiki_train.csv      # Training Corpus (~14k articles) [cite: 38]
â”‚   â”œâ”€â”€ wiki_test.csv       # In-domain test set [cite: 39]
â”‚   â”œâ”€â”€ arxiv_test.csv      # Scientific domain test set [cite: 39]
â”‚   â””â”€â”€ financial_test.csv  # Financial domain test set [cite: 39]
â”œâ”€â”€ main.py                 # Entry point: Orchestrates training and evaluation
â”œâ”€â”€ models.py               # Logic for N-gram counting, Laplace, and Good-Turing smoothing
â”œâ”€â”€ preprocessing.py        # Text cleaning, tokenization, and <UNK> replacement
â”œâ”€â”€ evaluation.py           # Perplexity calculation and report generation
â””â”€â”€ README.md               # Project documentation


Gemini said
Here is the complete, single-file README.md content. You can copy this entirely and paste it into your repository.

Markdown
# N-Gram Language Modeling & Domain Shift Analysis

## ğŸ“Œ Project Overview
This project implements **N-gram Language Models (Unigram, Bigram, Trigram)** from scratch for **CS60075 Natural Language Processing (Assignment 1)**[cite: 1].

The goal is to predict the next word in a sequence based on probability estimates derived from a training corpus[cite: 16]. The models are trained on Wikipedia articles and evaluated on three different domains to analyze the impact of **domain shift** on model performance[cite: 41].

Key features include:
* **Maximum Likelihood Estimation (MLE)** for probability calculation[cite: 51].
* **Smoothing Techniques**: Implementation of Laplace (Add-1)  and Good-Turing Discounting [cite: 65] to handle unseen n-grams.
* **Perplexity Evaluation**: Comparing model performance across In-Domain (Wikipedia) and Out-of-Domain (Scientific, Financial) datasets[cite: 104, 105].

---

## ğŸ“‚ Repository Structure

The codebase is modularized for efficiency and readability:

```text
â”œâ”€â”€ data/                   # Dataset folder (Place CSVs here)
â”‚   â”œâ”€â”€ wiki_train.csv      # Training Corpus (~14k articles) [cite: 38]
â”‚   â”œâ”€â”€ wiki_test.csv       # In-domain test set [cite: 39]
â”‚   â”œâ”€â”€ arxiv_test.csv      # Scientific domain test set [cite: 39]
â”‚   â””â”€â”€ financial_test.csv  # Financial domain test set [cite: 39]
â”œâ”€â”€ main.py                 # Entry point: Orchestrates training and evaluation
â”œâ”€â”€ models.py               # Logic for N-gram counting, Laplace, and Good-Turing smoothing
â”œâ”€â”€ preprocessing.py        # Text cleaning, tokenization, and <UNK> replacement
â”œâ”€â”€ evaluation.py           # Perplexity calculation and report generation
â””â”€â”€ README.md               # Project documentation

# âš™ï¸ Installation & Requirements
Ensure you have Python 3.x installed. The project relies on standard NLP and data libraries.   
Clone the repository:
Bash
git clone [https://github.com/Adityaraj142857/NLP.git](https://github.com/Adityaraj142857/NLP.git)
cd NLP
Install dependencies:
Bash
pip install pandas numpy nltk tqdm
NLTK Data: The script automatically checks for and downloads required NLTK resources (stopwords, wordnet, punkt) if they are missing.

